"""
Meta-Strategist: LLM-based autonomous instruction generation

The Meta-Strategist observes gameplay and generates high-level strategic
instructions that guide the RL reasoning neuron's decision-making.

This creates a two-level reasoning system:
1. Meta-Strategist (slow, strategic) - Generates instructions every N cycles
2. RL Reasoning Neuron (fast, tactical) - Follows instructions for immediate actions

Philosophy: Like a chess player thinking "control the center" (strategy)
while considering individual moves (tactics).
"""

import asyncio
from typing import Optional, List, Dict, Any
from dataclasses import dataclass
from datetime import datetime


@dataclass
class StrategicInstruction:
    """A strategic instruction generated by the meta-strategist."""
    instruction: str
    reasoning: str
    priority: str  # 'critical', 'important', 'suggested'
    duration_cycles: int  # How many cycles this instruction should remain active
    generated_at: int  # Cycle number when generated
    context: Dict[str, Any]  # Game state context when generated


class MetaStrategist:
    """
    Autonomous strategic instruction generator using LLM.
    
    Observes gameplay patterns and generates high-level strategic guidance
    that influences the RL reasoning neuron's tactical decisions.
    """
    
    def __init__(self, llm_interface=None, instruction_frequency: int = 10):
        """
        Initialize meta-strategist.
        
        Args:
            llm_interface: LLM interface for generating instructions
            instruction_frequency: Generate new instructions every N cycles
        """
        self.llm_interface = llm_interface
        self.instruction_frequency = instruction_frequency
        
        # Current strategic instructions
        self.active_instructions: List[StrategicInstruction] = []
        
        # History for learning
        self.instruction_history: List[StrategicInstruction] = []
        self.instruction_outcomes: Dict[str, List[float]] = {}
        
        # Gameplay observation
        self.recent_states: List[Dict[str, Any]] = []
        self.recent_actions: List[str] = []
        self.recent_rewards: List[float] = []
        self.max_history = 20
        
        # Cycle tracking
        self.current_cycle = 0
        self.last_instruction_cycle = 0
        
    def observe(
        self,
        state: Dict[str, Any],
        action: str,
        reward: float
    ):
        """
        Observe a gameplay step.
        
        Args:
            state: Current game state
            action: Action taken
            reward: Reward received
        """
        self.recent_states.append(state)
        self.recent_actions.append(action)
        self.recent_rewards.append(reward)
        
        # Keep only recent history
        if len(self.recent_states) > self.max_history:
            self.recent_states.pop(0)
            self.recent_actions.pop(0)
            self.recent_rewards.pop(0)
        
        self.current_cycle += 1
    
    async def should_generate_instruction(self) -> bool:
        """
        Determine if it's time to generate new strategic instructions.
        
        Returns:
            True if should generate new instruction
        """
        # Generate every N cycles
        cycles_since_last = self.current_cycle - self.last_instruction_cycle
        
        if cycles_since_last >= self.instruction_frequency:
            return True
        
        # Also generate if performance is poor
        if len(self.recent_rewards) >= 5:
            avg_recent_reward = sum(self.recent_rewards[-5:]) / 5
            if avg_recent_reward < 0.2:
                print("[META] ðŸ“‰ Poor performance detected, generating new strategy...")
                return True
        
        return False
    
    async def generate_instruction(
        self,
        current_state: Dict[str, Any],
        q_values: Dict[str, float],
        motivation: str
    ) -> Optional[StrategicInstruction]:
        """
        Generate a strategic instruction based on recent gameplay.
        
        Args:
            current_state: Current game state
            q_values: Current Q-values from RL
            motivation: Dominant motivation drive
            
        Returns:
            Strategic instruction or None if generation fails
        """
        if self.llm_interface is None:
            return self._heuristic_instruction(current_state, q_values, motivation)
        
        # Build meta-strategic prompt
        prompt = self._build_meta_prompt(current_state, q_values, motivation)
        
        try:
            # Query LLM for strategic instruction
            response = await self.llm_interface.client.generate(
                prompt=prompt,
                system_prompt=self._get_system_prompt(),
                temperature=0.8,  # Higher temp for creative strategies
                max_tokens=256
            )
            
            # Parse response
            instruction = self._parse_instruction_response(
                response['content'],
                current_state
            )
            
            if instruction:
                self.active_instructions.append(instruction)
                self.last_instruction_cycle = self.current_cycle
                
                print(f"\n[META] ðŸ§  New Strategy Generated:")
                print(f"[META]   Priority: {instruction.priority}")
                print(f"[META]   Instruction: {instruction.instruction}")
                print(f"[META]   Reasoning: {instruction.reasoning}")
                print(f"[META]   Duration: {instruction.duration_cycles} cycles\n")
                
                return instruction
            
        except Exception as e:
            print(f"[META] Instruction generation failed: {e}")
            return self._heuristic_instruction(current_state, q_values, motivation)
    
    def _get_system_prompt(self) -> str:
        """Get system prompt for meta-strategist."""
        return """You are a Meta-Strategist for an autonomous Skyrim AI agent.

Your role is to observe gameplay patterns and generate HIGH-LEVEL strategic instructions that guide the agent's behavior over multiple actions.

Think like a coach giving guidance to a player:
- "Focus on exploration and avoid unnecessary combat"
- "Build up resources before attempting difficult challenges"
- "Learn the environment layout before engaging enemies"

Your instructions should be:
1. Strategic (not tactical) - Guide overall approach, not individual moves
2. Actionable - Clear enough to influence decision-making
3. Time-bounded - Specify how long the strategy should be followed
4. Context-aware - Based on current situation and recent performance

Output format:
INSTRUCTION: [clear strategic guidance]
REASONING: [why this strategy makes sense now]
PRIORITY: [critical/important/suggested]
DURATION: [number of cycles to follow this strategy]"""
    
    def _build_meta_prompt(
        self,
        current_state: Dict[str, Any],
        q_values: Dict[str, float],
        motivation: str
    ) -> str:
        """Build prompt for meta-strategic reasoning."""
        # Analyze recent performance
        avg_reward = sum(self.recent_rewards) / len(self.recent_rewards) if self.recent_rewards else 0.0
        action_diversity = len(set(self.recent_actions)) if self.recent_actions else 0
        
        # Get top Q-values
        top_actions = sorted(q_values.items(), key=lambda x: x[1], reverse=True)[:3]
        q_summary = ", ".join([f"{a}={v:.2f}" for a, v in top_actions])
        
        prompt = f"""GAMEPLAY OBSERVATION:

Current Situation:
- Scene: {current_state.get('scene', 'unknown')}
- Health: {current_state.get('health', 100):.0f}/100
- In Combat: {current_state.get('in_combat', False)}
- Location: {current_state.get('location_name', 'Unknown')}
- Dominant Drive: {motivation}

Recent Performance (last {len(self.recent_rewards)} cycles):
- Average Reward: {avg_reward:.2f}
- Action Diversity: {action_diversity} unique actions
- Recent Actions: {', '.join(self.recent_actions[-5:]) if self.recent_actions else 'none'}

Learned Q-Values (what RL has learned works):
- Top Actions: {q_summary}

Current Active Instructions:
"""
        
        if self.active_instructions:
            for instr in self.active_instructions:
                cycles_active = self.current_cycle - instr.generated_at
                prompt += f"- [{instr.priority}] {instr.instruction} (active {cycles_active}/{instr.duration_cycles} cycles)\n"
        else:
            prompt += "- None (agent needs strategic guidance)\n"
        
        prompt += """
Based on this observation, generate a NEW strategic instruction that will improve the agent's gameplay over the next several cycles.

Consider:
1. Is the agent stuck in repetitive behavior?
2. Is performance declining?
3. Are there unexplored opportunities?
4. Does the current strategy need adjustment?

Generate your strategic instruction now:"""
        
        return prompt
    
    def _parse_instruction_response(
        self,
        response: str,
        context: Dict[str, Any]
    ) -> Optional[StrategicInstruction]:
        """Parse LLM response into structured instruction."""
        instruction_text = ""
        reasoning = ""
        priority = "important"
        duration = 10
        
        lines = response.split('\n')
        for line in lines:
            line = line.strip()
            
            if line.startswith('INSTRUCTION:'):
                instruction_text = line[12:].strip()
            elif line.startswith('REASONING:'):
                reasoning = line[10:].strip()
            elif line.startswith('PRIORITY:'):
                priority_text = line[9:].strip().lower()
                if 'critical' in priority_text:
                    priority = 'critical'
                elif 'suggested' in priority_text:
                    priority = 'suggested'
                else:
                    priority = 'important'
            elif line.startswith('DURATION:'):
                try:
                    duration = int(''.join(filter(str.isdigit, line)))
                except ValueError:
                    duration = 10
        
        # Fallback: extract from unstructured response
        if not instruction_text:
            # Use first sentence as instruction
            sentences = response.split('.')
            if sentences:
                instruction_text = sentences[0].strip()
                reasoning = response.strip()
        
        if instruction_text:
            return StrategicInstruction(
                instruction=instruction_text,
                reasoning=reasoning or "Strategic guidance based on current gameplay",
                priority=priority,
                duration_cycles=duration,
                generated_at=self.current_cycle,
                context=context
            )
        
        return None
    
    def _heuristic_instruction(
        self,
        current_state: Dict[str, Any],
        q_values: Dict[str, float],
        motivation: str
    ) -> StrategicInstruction:
        """Generate heuristic instruction when LLM unavailable."""
        # Simple heuristic strategies
        health = current_state.get('health', 100)
        in_combat = current_state.get('in_combat', False)
        
        if health < 30:
            instruction = "Prioritize survival: avoid combat and seek healing"
            priority = 'critical'
        elif in_combat:
            instruction = "Focus on combat effectiveness: use learned combat actions"
            priority = 'important'
        elif motivation == 'curiosity':
            instruction = "Explore systematically: discover new areas and interactions"
            priority = 'suggested'
        else:
            instruction = "Balanced approach: explore while maintaining readiness"
            priority = 'suggested'
        
        return StrategicInstruction(
            instruction=instruction,
            reasoning="Heuristic strategy based on current state",
            priority=priority,
            duration_cycles=10,
            generated_at=self.current_cycle,
            context=current_state
        )
    
    def get_active_instruction_context(self) -> str:
        """Get formatted context of active instructions for RL reasoning."""
        if not self.active_instructions:
            return ""
        
        # Remove expired instructions
        self.active_instructions = [
            instr for instr in self.active_instructions
            if (self.current_cycle - instr.generated_at) < instr.duration_cycles
        ]
        
        if not self.active_instructions:
            return ""
        
        context = "\nðŸ§  META-STRATEGIC GUIDANCE (follow this high-level strategy):\n"
        
        for instr in self.active_instructions:
            cycles_remaining = instr.duration_cycles - (self.current_cycle - instr.generated_at)
            priority_emoji = {
                'critical': 'ðŸ”´',
                'important': 'ðŸŸ¡',
                'suggested': 'ðŸŸ¢'
            }[instr.priority]
            
            context += f"{priority_emoji} [{cycles_remaining} cycles left] {instr.instruction}\n"
            context += f"   Reasoning: {instr.reasoning}\n"
        
        context += "\nAlign your tactical decisions with this strategic guidance.\n"
        
        return context
    
    def get_stats(self) -> Dict[str, Any]:
        """Get meta-strategist statistics."""
        return {
            'active_instructions': len(self.active_instructions),
            'total_generated': len(self.instruction_history),
            'current_cycle': self.current_cycle,
            'cycles_since_last': self.current_cycle - self.last_instruction_cycle
        }


# Example usage
if __name__ == "__main__":
    strategist = MetaStrategist()
    
    # Simulate observations
    for i in range(15):
        strategist.observe(
            state={'health': 100 - i*5, 'scene': 'combat'},
            action='move_forward',
            reward=0.1
        )
    
    # Check if should generate
    print(f"Should generate: {asyncio.run(strategist.should_generate_instruction())}")
    
    # Generate heuristic instruction
    instr = strategist._heuristic_instruction(
        {'health': 25, 'in_combat': True},
        {'explore': 0.5, 'combat': 0.3},
        'autonomy'
    )
    print(f"\nGenerated: {instr.instruction}")
    print(f"Priority: {instr.priority}")
