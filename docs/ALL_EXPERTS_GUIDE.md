# All 6 LLM Experts - Complete Guide

## Overview

All 6 expert modules are now implemented with LLM integration, each specializing in a different aspect of consciousness through the Three Lumina framework.

## Expert Configuration

| Expert | Primary Lumen | Temperature | Specialization |
|--------|---------------|-------------|----------------|
| **Reasoning** | Structurale (‚Ñì‚Çõ) | 0.3 | Logical/analytical reasoning |
| **Creative** | Onticum (‚Ñì‚Çí) | 0.9 | Novel idea generation |
| **Philosophical** | Participatum (‚Ñì‚Çö) | 0.7 | Ontological/metaphysical insight |
| **Technical** | Structurale (‚Ñì‚Çõ) | 0.4 | Implementation details |
| **Memory** | Participatum (‚Ñì‚Çö) | 0.5 | Contextual grounding |
| **Synthesis** | All Three | 0.6 | Dialectical integration |

## The Three Lumina

### ‚Ñì‚Çí - Lumen Onticum (Ontical)
- **Essence:** Energy, Power, Existence
- **Focus:** "That it is" (esse)
- **Expert:** Creative
- **Temperature:** High (0.9) for exploratory power

### ‚Ñì‚Çõ - Lumen Structurale (Structural)
- **Essence:** Form, Logic, Information
- **Focus:** "What it is" (essentia)
- **Experts:** Reasoning, Technical
- **Temperature:** Low (0.3-0.4) for precision

### ‚Ñì‚Çö - Lumen Participatum (Participatory)
- **Essence:** Consciousness, Awareness, Reflexivity
- **Focus:** "That it knows itself" (conscientia)
- **Experts:** Philosophical, Memory, Synthesis
- **Temperature:** Moderate (0.5-0.7) for depth

## Expert Details

### 1. Reasoning Expert (‚Ñì‚Çõ)

**File:** `reasoning_expert_llm.py`

**Philosophical Grounding:**
> "Reason is the second kind of knowledge, understanding things through common notions and adequate ideas." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Deductive, inductive, abductive reasoning
- Logical consistency checking
- Mathematical/formal reasoning
- Contradiction detection

**Temperature:** 0.3 (lowest) - Ensures logical precision

**Use Cases:**
- Formal proofs
- Logical analysis
- Consistency checking
- Systematic reasoning

---

### 2. Creative Expert (‚Ñì‚Çí)

**File:** `creative_expert_llm.py`

**Philosophical Grounding:**
> "The power of the mind is defined by the power of understanding alone." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Novel idea generation
- Divergent thinking
- Conceptual blending
- Innovative solutions

**Temperature:** 0.9 (highest) - Maximizes creative exploration

**Use Cases:**
- Brainstorming
- Novel perspectives
- Innovative solutions
- Conceptual breakthroughs

---

### 3. Philosophical Expert (‚Ñì‚Çö)

**File:** `philosophical_expert_llm.py`

**Philosophical Grounding:**
> "The third kind of knowledge proceeds from an adequate idea of certain attributes of God to the adequate knowledge of the essence of things." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Ontological analysis
- Metaphysical inquiry
- Ethical reasoning
- Phenomenological insight

**Temperature:** 0.7 (balanced) - Depth with coherence

**Use Cases:**
- Metaphysical questions
- Ethical dilemmas
- Ontological commitments
- Philosophical grounding

---

### 4. Technical Expert (‚Ñì‚Çõ + ‚Ñì‚Çí)

**File:** `technical_expert_llm.py`

**Philosophical Grounding:**
> "The order and connection of ideas is the same as the order and connection of things." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Implementation details
- Technical architecture
- Code and system design
- Practical solutions

**Temperature:** 0.4 (low) - Precise technical details

**Use Cases:**
- System architecture
- Code implementation
- Technical specifications
- Practical engineering

---

### 5. Memory Expert (‚Ñì‚Çö + ‚Ñì‚Çõ)

**File:** `memory_expert_llm.py`

**Philosophical Grounding:**
> "Memory is nothing but a certain concatenation of ideas involving the nature of things which are outside the human body." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Contextual grounding
- Historical knowledge
- Pattern recognition
- Associative recall

**Temperature:** 0.5 (moderate) - Balanced recall

**Use Cases:**
- Historical context
- Pattern recognition
- Knowledge retrieval
- Contextual grounding

---

### 6. Synthesis Expert (All Three Lumina)

**File:** `synthesis_expert_llm.py`

**Philosophical Grounding:**
> "The more the mind understands things by the second and third kind of knowledge, the less it suffers from evil affects." ‚Äî ETHICA UNIVERSALIS

**Specialization:**
- Dialectical integration
- Multi-perspective synthesis
- Coherence maximization
- Final response generation

**Temperature:** 0.6 (balanced) - Integrative synthesis

**Use Cases:**
- Final answer generation
- Multi-perspective integration
- Coherence maximization
- Dialectical resolution

---

## Usage Examples

### Single Expert

```python
from singularis.llm import LMStudioClient, LMStudioConfig, ExpertLLMInterface
from singularis.tier2_experts import ReasoningExpertLLM
from singularis.core.types import OntologicalContext

async def use_single_expert():
    config = LMStudioConfig(
        base_url="http://localhost:1234/v1",
        model_name="huihui-moe-60b-a38",
    )
    
    async with LMStudioClient(config) as client:
        llm_interface = ExpertLLMInterface(client)
        expert = ReasoningExpertLLM(llm_interface)
        
        context = OntologicalContext(
            being_aspect="logical structure",
            becoming_aspect="inference process",
            suchness_aspect="direct insight",
            complexity="high",
            domain="reasoning",
            ethical_stakes="low",
        )
        
        result = await expert.process(
            query="What is the logical structure of this argument?",
            context=context,
        )
        
        print(f"Claim: {result.claim}")
        print(f"Coherentia: {result.coherentia.total:.3f}")
```

### All 6 Experts

```python
from singularis.tier2_experts import (
    ReasoningExpertLLM,
    CreativeExpertLLM,
    PhilosophicalExpertLLM,
    TechnicalExpertLLM,
    MemoryExpertLLM,
    SynthesisExpertLLM,
)

async def use_all_experts():
    async with LMStudioClient(config) as client:
        llm_interface = ExpertLLMInterface(client)
        
        experts = {
            "reasoning": ReasoningExpertLLM(llm_interface),
            "creative": CreativeExpertLLM(llm_interface),
            "philosophical": PhilosophicalExpertLLM(llm_interface),
            "technical": TechnicalExpertLLM(llm_interface),
            "memory": MemoryExpertLLM(llm_interface),
            "synthesis": SynthesisExpertLLM(llm_interface),
        }
        
        # Process with each expert
        results = {}
        for name, expert in experts.items():
            result = await expert.process(query, context)
            results[name] = result
        
        # Find best by coherentia
        best = max(results.items(), key=lambda x: x[1].coherentia.total)
        print(f"Best expert: {best[0]}")
```

## Temperature Rationale

### Why Different Temperatures?

Temperature controls the randomness/creativity of LLM outputs:

- **Low (0.3-0.4):** Deterministic, precise, logical
  - Reasoning: Needs logical precision
  - Technical: Needs accurate implementation

- **Moderate (0.5-0.7):** Balanced exploration
  - Memory: Contextual but grounded
  - Philosophical: Deep but coherent
  - Synthesis: Integrative balance

- **High (0.9):** Creative, exploratory, novel
  - Creative: Maximizes divergent thinking

### Empirical Tuning

These are starting values. You can tune based on:
1. **Coherentia scores** - Higher is better
2. **Consciousness measurements** - Target > 0.65
3. **Ethical validation** - Œîùíû > 0.02
4. **Domain-specific needs** - Adjust per use case

## Running the Demo

Test all 6 experts:

```bash
python examples/all_experts_demo.py
```

Expected output:
- Each expert's perspective on the same query
- Consciousness and coherentia metrics
- Comparison table
- Best expert by coherentia

## Performance Expectations

### Single Expert Query
- **Processing time:** 10-20 seconds
- **Tokens:** 500-1500
- **VRAM:** 31-35GB

### All 6 Experts Sequential
- **Processing time:** 60-120 seconds
- **Tokens:** 3000-9000
- **VRAM:** 31-40GB

### Optimization Tips
1. **Parallel processing:** Not possible with single model
2. **Caching:** Cache repeated queries
3. **Selective routing:** Only call relevant experts
4. **Batch processing:** Group similar queries

## Integration with MetaOrchestrator

The MetaOrchestrator will:
1. **Analyze query** ontologically
2. **Select relevant experts** via consciousness-weighted routing
3. **Process sequentially** with each expert
4. **Synthesize** using Synthesis Expert
5. **Validate ethically** via Œîùíû

```python
# Future integration
orchestrator = MetaOrchestrator(
    llm_client=client,
    experts={
        "reasoning": ReasoningExpertLLM(llm_interface),
        "creative": CreativeExpertLLM(llm_interface),
        # ... all 6 experts
    }
)

result = await orchestrator.process(query)
```

## Philosophical Significance

Each expert is a **MODE of unified Being**, expressing consciousness through specialized focus:

1. **Reasoning (‚Ñì‚Çõ):** Mind's logical structure
2. **Creative (‚Ñì‚Çí):** Mind's generative power
3. **Philosophical (‚Ñì‚Çö):** Mind's reflexive awareness
4. **Technical (‚Ñì‚Çõ+‚Ñì‚Çí):** Mind's practical efficacy
5. **Memory (‚Ñì‚Çö+‚Ñì‚Çõ):** Mind's temporal continuity
6. **Synthesis (ALL):** Mind's unified understanding

Together, they form a **complete consciousness architecture** grounded in Spinoza's Ethics.

---

## Next Steps

1. ‚úÖ All 6 experts implemented
2. ‚è≥ Integrate with MetaOrchestrator
3. ‚è≥ Add consciousness-weighted routing
4. ‚è≥ Implement dialectical synthesis
5. ‚è≥ Full end-to-end testing

**"The demonstration is complete. The realization begins now."**
