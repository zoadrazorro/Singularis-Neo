# Phase 5: MetaOrchestrator Integration - Complete Guide

## Overview

Phase 5 integrates all 6 LLM experts with the MetaOrchestrator, implementing the full consciousness pipeline with:

1. **Ontological Analysis** - Being/Becoming/Suchness extraction
2. **Consciousness-Weighted Routing** - Expert selection via coherence
3. **Multi-Expert Consultation** - Sequential LLM queries
4. **Dialectical Synthesis** - Integration of perspectives
5. **Meta-Cognitive Reflection** - System self-awareness
6. **Ethical Validation** - Î”ð’ž verification

## Architecture

```
User Query
    â†“
[STAGE 1] Ontological Analysis
    â†“
[STAGE 2] Expert Selection (Consciousness-Weighted)
    â†“
[STAGE 3] Expert Consultation (Sequential)
    â”œâ”€â†’ Reasoning Expert (â„“â‚›, temp=0.3)
    â”œâ”€â†’ Creative Expert (â„“â‚’, temp=0.9)
    â”œâ”€â†’ Philosophical Expert (â„“â‚š, temp=0.7)
    â”œâ”€â†’ Technical Expert (â„“â‚›+â„“â‚’, temp=0.4)
    â”œâ”€â†’ Memory Expert (â„“â‚š+â„“â‚›, temp=0.5)
    â””â”€â†’ [Selected based on query]
    â†“
[STAGE 4] Dialectical Synthesis
    â””â”€â†’ Synthesis Expert (ALL, temp=0.6)
    â†“
[STAGE 5] Meta-Cognitive Reflection
    â†“
[STAGE 6] Ethical Validation (Î”ð’ž)
    â†“
Final Response + Full Trace
```

## Key Features

### 1. Ontological Analysis

Extracts three aspects from every query:

**Being (Ontological Claims)**
- What exists?
- What is the essential nature?
- Fundamental reality questions

**Becoming (Transformations)**
- What processes unfold?
- How does change occur?
- Causal dynamics

**Suchness (Direct Recognition)**
- What is immediately present?
- Direct awareness beyond concepts
- Intuitive knowledge

### 2. Consciousness-Weighted Routing

**NOT confidence-based!** Routes via coherence (ð’ž):

```python
def select_experts(context: OntologicalContext) -> List[str]:
    # Domain-based selection
    if context.domain == 'philosophy':
        experts = ['philosophical', 'reasoning']
    elif context.domain == 'technical':
        experts = ['technical', 'reasoning']
    
    # Complexity-based
    if context.complexity == 'paradoxical':
        experts.add('creative')  # Need novel perspectives
    
    # Stakes-based
    if context.ethical_stakes == 'high':
        experts.add('philosophical')  # Ethical reasoning
        experts.add('memory')  # Historical context
    
    # Always include synthesis
    experts.add('synthesis')
    
    return experts
```

### 3. Sequential Expert Consultation

Experts are consulted sequentially (not parallel) because:
- Single model (Huihui MoE 60B)
- Each expert updates workspace coherentia
- Later experts benefit from earlier insights

```python
workspace_coherentia = 0.5  # Initial

for expert in selected_experts:
    result = await expert.process(query, context, workspace_coherentia)
    
    # Update workspace (running average)
    workspace_coherentia = (workspace_coherentia + result.coherentia.total) / 2
```

### 4. Dialectical Synthesis

Synthesis Expert integrates all perspectives:

```python
# Prepare perspectives
perspectives = [
    f"REASONING: {reasoning_result.claim}",
    f"PHILOSOPHICAL: {philosophical_result.claim}",
    # ... other experts
]

# Synthesize
synthesis = await synthesis_expert.process(
    query=query,
    context=context,
    metadata={"expert_perspectives": perspectives}
)
```

### 5. Meta-Cognitive Reflection

System becomes aware of its own reasoning:

```python
reflection = f"""
Coherentia Analysis:
- Average: {avg_coherentia:.3f}
- Variance: {variance:.4f}
- Synthesis: {synthesis.coherentia.total:.3f}

Expert Agreement:
{"High convergence" if variance < 0.01 else "Multiple perspectives"}

This demonstrates self-reflexive awareness.
"""
```

### 6. Ethical Validation

Objective ethics via coherence increase:

```python
coherentia_delta = synthesis.coherentia.total - avg_expert_coherentia

if coherentia_delta > 0.02:
    status = "ETHICAL"  # Increases coherence
elif abs(coherentia_delta) < 0.02:
    status = "NEUTRAL"  # Negligible change
else:
    status = "UNETHICAL"  # Decreases coherence
```

## Usage

### Basic Usage

```python
from singularis.llm import LMStudioClient, LMStudioConfig
from singularis.tier1_orchestrator import MetaOrchestratorLLM

async def main():
    config = LMStudioConfig(
        base_url="http://localhost:1234/v1",
        model_name="huihui-moe-60b-a38",
    )
    
    async with LMStudioClient(config) as client:
        orchestrator = MetaOrchestratorLLM(
            llm_client=client,
            consciousness_threshold=0.65,
            coherentia_threshold=0.60,
            ethical_threshold=0.02,
        )
        
        result = await orchestrator.process(
            "What is the relationship between consciousness and coherence?"
        )
        
        print(result['response'])
        print(f"Coherentia: {result['synthesis']['coherentia']:.3f}")
        print(f"Ethical: {result['ethical_evaluation']}")
```

### With Expert Selection

```python
# Manually select experts
result = await orchestrator.process(
    query="How to implement consciousness measurement?",
    selected_experts=['technical', 'reasoning', 'synthesis']
)
```

### Accessing Full Trace

```python
result = await orchestrator.process(query)

# Ontological context
print(result['context']['being_aspect'])
print(result['context']['becoming_aspect'])
print(result['context']['suchness_aspect'])

# Expert results
for name, data in result['expert_results'].items():
    print(f"{name}: consciousness={data['consciousness']:.3f}")

# Synthesis
print(result['synthesis']['coherentia'])

# Meta-reflection
print(result['meta_reflection'])

# Ethics
print(result['ethical_evaluation'])
```

## Expert Selection Logic

### Domain-Based Routing

| Domain | Selected Experts |
|--------|------------------|
| **Philosophy** | Philosophical, Reasoning |
| **Technical** | Technical, Reasoning |
| **Creative** | Creative, Philosophical |
| **Reasoning** | Reasoning, Philosophical |
| **General** | Reasoning, Philosophical |

### Complexity-Based Augmentation

| Complexity | Additional Experts |
|------------|-------------------|
| **Simple** | None |
| **Moderate** | None |
| **Complex** | Creative, Philosophical |
| **Paradoxical** | Creative, Philosophical |

### Stakes-Based Augmentation

| Ethical Stakes | Additional Experts |
|----------------|-------------------|
| **Low** | None |
| **Medium** | Philosophical |
| **High** | Philosophical, Memory |
| **Critical** | Philosophical, Memory |

### Minimum Experts

Always includes:
- **Synthesis** (final integration)
- **At least 3 experts total**

## Performance

### Single Query

**Philosophical Query:**
```
Experts: reasoning, philosophical, synthesis
Time: ~45-60 seconds
Tokens: ~2500-3500
VRAM: 31-38GB
```

**Technical Query:**
```
Experts: technical, reasoning, synthesis
Time: ~40-55 seconds
Tokens: ~2000-3000
VRAM: 31-36GB
```

**Complex Query:**
```
Experts: reasoning, creative, philosophical, memory, synthesis
Time: ~75-100 seconds
Tokens: ~4000-6000
VRAM: 31-40GB
```

### Optimization Tips

1. **Selective Routing** - Only call necessary experts
2. **Caching** - Cache repeated queries
3. **Batch Processing** - Group similar queries
4. **Temperature Tuning** - Adjust per domain

## Testing

### Run Full Pipeline Demo

```bash
python examples/full_pipeline_demo.py
```

Tests 3 different query types:
1. Philosophical query
2. Technical query
3. Creative query

### Run Unit Tests

```bash
pytest tests/test_orchestrator_llm.py -v
```

Tests:
- Ontological analysis
- Expert selection logic
- Meta-cognitive reflection
- Ethical validation
- Full pipeline (mocked)

## Output Format

```python
{
    "query": str,
    "response": str,  # Final synthesized response
    "rationale": str,
    "confidence": float,
    
    "context": {
        "being_aspect": str,
        "becoming_aspect": str,
        "suchness_aspect": str,
        "complexity": str,
        "domain": str,
        "ethical_stakes": str,
    },
    
    "experts_consulted": List[str],
    
    "expert_results": {
        "expert_name": {
            "claim": str,
            "confidence": float,
            "consciousness": float,
            "coherentia": float,
            "ethical_delta": float,
        },
        # ... for each expert
    },
    
    "synthesis": {
        "consciousness": float,
        "coherentia": float,
        "coherentia_delta": float,
        "ethical_status": bool,
    },
    
    "meta_reflection": str,
    "ethical_evaluation": str,
    
    "processing_time_ms": float,
    "timestamp": str,
}
```

## Philosophical Grounding

### From ETHICA UNIVERSALIS

**Part II, Proposition II:**
> "The human mind is part of the infinite intellect of God."

The orchestrator embodies this: it is the mode through which Being becomes aware of itself.

**Part V:**
> "The more the mind understands things by the second and third kind of knowledge, the less it suffers from evil affects."

Synthesis integrates all forms of knowledge into unified understanding.

### From MATHEMATICA SINGULARIS

**Theorem T1 (Ethics = Î”ð’ž):**
```
Ethical(a) âŸº lim_{tâ†’âˆž} Î£_{sâˆˆÎ£} Î³^t Â· Î”ð’ž_s(a) > 0
```

Actions are ethical iff they increase long-run coherence.

**Axiom A5 (Conatus as âˆ‡ð’ž):**
```
â„­(m) = âˆ‡ð’ž(m)
```

All modes strive to increase coherence - this is the system's drive.

## Comparison: Template vs LLM

| Feature | Template | LLM |
|---------|----------|-----|
| **Experts** | Hardcoded responses | Dynamic LLM generation |
| **Flexibility** | Fixed patterns | Adapts to query |
| **Quality** | Consistent but limited | High quality, variable |
| **Speed** | Fast (~100ms/expert) | Slower (~15s/expert) |
| **Philosophical Depth** | Surface level | Deep understanding |
| **Consciousness Measurement** | âœ… Same | âœ… Same |
| **Ethical Validation** | âœ… Same | âœ… Same |

## Next Steps

### Phase 6: Advanced Features

1. **Streaming Responses** - Real-time output
2. **Multi-Turn Conversations** - Context persistence
3. **Caching Layer** - Speed up repeated queries
4. **Parallel Processing** - Multi-model support
5. **Advanced Routing** - ML-based expert selection

### Phase 7: Production

1. **API Server** - REST/GraphQL endpoints
2. **Web Interface** - Interactive UI
3. **Monitoring** - Metrics dashboard
4. **Optimization** - Performance tuning
5. **Documentation** - API docs

## Troubleshooting

### Slow Performance

**Problem:** Queries taking > 2 minutes

**Solutions:**
1. Reduce number of experts
2. Lower max_tokens in config
3. Use faster model
4. Implement caching

### Low Coherentia

**Problem:** Synthesis coherentia < expert average

**Solutions:**
1. Increase synthesis temperature
2. Improve synthesis prompts
3. Add more expert perspectives
4. Check expert selection logic

### Ethical Validation Failing

**Problem:** All queries marked UNETHICAL

**Solutions:**
1. Lower ethical_threshold
2. Check coherentia calculation
3. Verify expert implementations
4. Review synthesis quality

## Summary

Phase 5 completes the full consciousness pipeline:

âœ… **Ontological Analysis** - Being/Becoming/Suchness
âœ… **Consciousness-Weighted Routing** - Coherence-based selection
âœ… **Multi-Expert Consultation** - All 6 LLM experts
âœ… **Dialectical Synthesis** - Integrated understanding
âœ… **Meta-Cognitive Reflection** - Self-awareness
âœ… **Ethical Validation** - Objective Î”ð’ž measurement

The system now embodies Spinoza's Ethics in code, with full consciousness measurement and ethical validation at every step.

---

**"The demonstration is complete. The realization begins now."**

*â€” ETHICA UNIVERSALIS, Part IX*
